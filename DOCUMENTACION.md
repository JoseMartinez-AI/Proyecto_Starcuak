# üìö Documentaci√≥n T√©cnica - Proyecto Starcuak

## üìã √çndice
1. [Visi√≥n General del Proyecto](#visi√≥n-general-del-proyecto)
2. [Arquitectura del Sistema](#arquitectura-del-sistema)
3. [M√≥dulos y Componentes](#m√≥dulos-y-componentes)
4. [Documentaci√≥n Detallada por Archivo](#documentaci√≥n-detallada-por-archivo)
5. [Flujo de Datos](#flujo-de-datos)
6. [Instalaci√≥n y Configuraci√≥n](#instalaci√≥n-y-configuraci√≥n)
7. [Casos de Uso](#casos-de-uso)

---

## üéØ Visi√≥n General del Proyecto

**Starcuak Admin Pro** es una aplicaci√≥n web desarrollada con Streamlit que implementa un sistema de an√°lisis de sentimientos para rese√±as de productos de caf√©. El sistema utiliza modelos de inteligencia artificial basados en transformers para clasificar autom√°ticamente el sentimiento de los comentarios de clientes.

### Caracter√≠sticas Principales:
- ‚úÖ An√°lisis de sentimientos en tiempo real utilizando IA
- üìä Dashboard interactivo con visualizaciones avanzadas
- üíæ Base de datos SQLite para persistencia de datos
- üìÅ Carga masiva de datos mediante archivos CSV
- üìà M√©tricas y KPIs para an√°lisis de negocio
- üîç Sistema de b√∫squeda y filtrado de datos

### Tecnolog√≠as Utilizadas:
- **Frontend**: Streamlit
- **IA/ML**: Transformers (Hugging Face) con modelo BETO
- **Base de Datos**: SQLite3
- **An√°lisis de Datos**: Pandas
- **Visualizaci√≥n**: Matplotlib
- **Lenguaje**: Python 3.x

---

## üèóÔ∏è Arquitectura del Sistema

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    INTERFAZ STREAMLIT                        ‚îÇ
‚îÇ                         (app.py)                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ                   ‚îÇ                   ‚îÇ
         ‚ñº                   ‚ñº                   ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  AnalizadorIA   ‚îÇ  ‚îÇ   StarcuakDB    ‚îÇ  ‚îÇ  FileManager    ‚îÇ
‚îÇ  (ia_model.py)  ‚îÇ  ‚îÇ  (database.py)  ‚îÇ  ‚îÇ(file_manager.py)‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                   ‚îÇ                   ‚îÇ
         ‚îÇ                   ‚îÇ                   ‚îÇ
         ‚ñº                   ‚ñº                   ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Modelo BETO     ‚îÇ  ‚îÇ  SQLite Database‚îÇ  ‚îÇ  CSV / Logs     ‚îÇ
‚îÇ (Transformers)  ‚îÇ  ‚îÇ  (starcuak.db)  ‚îÇ  ‚îÇ  (data/outputs) ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Patr√≥n de Dise√±o:
El proyecto sigue el patr√≥n **MVC (Modelo-Vista-Controlador)** adaptado:
- **Vista**: Interfaz Streamlit (`app.py`)
- **Modelo**: Clases de negocio en el paquete `models/`
- **Controlador**: L√≥gica de interacci√≥n entre vista y modelo

---

## üì¶ M√≥dulos y Componentes

### Estructura de Directorios:
```
PROYECTO_STARCUAK/
‚îÇ
‚îú‚îÄ‚îÄ app.py                      # Aplicaci√≥n principal Streamlit
‚îú‚îÄ‚îÄ requirements.txt            # Dependencias del proyecto
‚îú‚îÄ‚îÄ README.md                   # Documentaci√≥n b√°sica
‚îÇ
‚îú‚îÄ‚îÄ models/                     # Paquete de modelos
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py            # Inicializador del paquete
‚îÇ   ‚îú‚îÄ‚îÄ database.py            # Gesti√≥n de base de datos
‚îÇ   ‚îú‚îÄ‚îÄ ia_model.py            # Modelo de IA para an√°lisis
‚îÇ   ‚îî‚îÄ‚îÄ file_manager.py        # Gesti√≥n de archivos
‚îÇ
‚îî‚îÄ‚îÄ data/                       # Datos y salidas
    ‚îú‚îÄ‚îÄ inputs/                # Archivos CSV de entrada
    ‚îÇ   ‚îú‚îÄ‚îÄ Datos_1.csv
    ‚îÇ   ‚îî‚îÄ‚îÄ Datos_2.csv
    ‚îú‚îÄ‚îÄ outputs/               # Archivos de salida
    ‚îÇ   ‚îî‚îÄ‚îÄ log.txt           # Registro de operaciones
    ‚îî‚îÄ‚îÄ starcuak.db           # Base de datos SQLite (generado)
```

---

## üìÑ Documentaci√≥n Detallada por Archivo

### 1Ô∏è‚É£ `app.py` - Aplicaci√≥n Principal

Este es el archivo principal que ejecuta la interfaz web de Streamlit y coordina todos los m√≥dulos del sistema.

#### **Secci√≥n 1: Importaciones y Configuraci√≥n Inicial (L√≠neas 1-12)**

```python
import streamlit as st
import matplotlib.pyplot as plt
import pandas as pd
from datetime import datetime

from models import StarcuakDB, AnalizadorIA, FileManager
```

**Explicaci√≥n:**
- Se importan las librer√≠as necesarias para la interfaz web, visualizaci√≥n y manejo de datos
- La importaci√≥n desde `models` utiliza el archivo `__init__.py` para centralizar las clases
- `datetime` se usa para el registro de fechas en los an√°lisis

**Instancias de Clases:**
```python
db = StarcuakDB()
ia = AnalizadorIA()
fm = FileManager()
```
- `db`: Instancia para gestionar la base de datos SQLite
- `ia`: Instancia del analizador de sentimientos con IA
- `fm`: Instancia para gesti√≥n de archivos (CSV, logs, backups)

#### **Secci√≥n 2: Configuraci√≥n de Streamlit (L√≠neas 15-22)**

```python
st.set_page_config(page_title="Starcuak Admin Pro", page_icon="‚òï", layout="wide")

if "modulo_seleccionado" not in st.session_state:
    st.session_state.modulo_seleccionado = "Nueva Rese√±a"

if "uploader_key" not in st.session_state:
    st.session_state.uploader_key = 0
```

**Explicaci√≥n:**
- **`st.set_page_config`**: Configura el t√≠tulo, icono y dise√±o de la aplicaci√≥n web
  - `layout="wide"`: Utiliza todo el ancho de la pantalla
- **`st.session_state`**: Mecanismo de Streamlit para mantener estado entre recargas
  - `modulo_seleccionado`: Almacena qu√© m√≥dulo est√° activo actualmente
  - `uploader_key`: Contador para resetear el cargador de archivos cuando sea necesario

#### **Secci√≥n 3: Barra Lateral y Navegaci√≥n (L√≠neas 25-32)**

```python
st.sidebar.title("‚òï Starcuak Panel")

opciones = ["Nueva Rese√±a", "Carga CSV", "Base de Datos", "Dashboard Pro"]
indice_actual = opciones.index(st.session_state.modulo_seleccionado)
menu = st.sidebar.selectbox(
    "M√≥dulo", opciones, index=indice_actual, key="menu_seleccion"
)
st.session_state.modulo_seleccionado = menu
```

**Explicaci√≥n:**
- Crea un men√∫ lateral con 4 opciones principales
- `indice_actual`: Mantiene sincronizado el estado con la selecci√≥n visual
- `st.sidebar.selectbox`: Componente desplegable para navegaci√≥n
- Actualiza el estado de sesi√≥n con la selecci√≥n actual

#### **Secci√≥n 4: Bot√≥n de Limpieza de Datos (L√≠neas 37-50)**

```python
if st.sidebar.button("üóëÔ∏è Limpiar Base de Datos"):
    try:
        db.limpiar_datos()
        fm.registrar_log("Base de datos limpiada por el usuario.")
        st.sidebar.success("¬°Base de datos vaciada!")

        st.session_state.uploader_key += 1
        st.session_state.modulo_seleccionado = "Base de Datos"
        st.rerun()
    except Exception as e:
        st.sidebar.error(f"Error al limpiar datos: {e}")
```

**Explicaci√≥n:**
- **Funcionalidad**: Elimina todos los registros de la base de datos
- **`db.limpiar_datos()`**: Ejecuta DELETE y reinicia el autoincremento
- **`fm.registrar_log()`**: Registra la operaci√≥n en el archivo log.txt
- **`st.session_state.uploader_key += 1`**: Fuerza el reseteo del cargador de archivos
- **`st.rerun()`**: Recarga la aplicaci√≥n para reflejar los cambios inmediatamente
- Manejo de excepciones para mostrar errores al usuario

#### **Secci√≥n 5: M√ìDULO 1 - Nueva Rese√±a Manual (L√≠neas 53-73)**

```python
if menu == "Nueva Rese√±a":
    st.header("üìù Nueva Rese√±a Manual")
    with st.form("form_manual", clear_on_submit=True):
        prod = st.selectbox("Caf√©", ["Espresso", "Americano", "Latte", "Capuccino"])
        txt = st.text_area("Comentario", placeholder="Escriba el comentario aqu√≠...")

        if st.form_submit_button("Analizar"):
            if txt.strip():
                label, score = ia.analizar(txt)
                current_date = datetime.now().strftime("%d/%m/%Y %H:%M")
                db.insertar_resena(prod, txt, label, score, current_date)
                fm.registrar_log(f"Analisis Manual: {prod} -> {label}")

                st.info(f"**Comentario procesado:**\n\n{txt}")
                st.success(f"**‚úÖ Sentimiento:** {label} | **Confianza:** {score:.2f}")
            else:
                st.warning("‚ö†Ô∏è El comentario no puede estar vac√≠o.")
```

**Explicaci√≥n:**
- **Prop√≥sito**: Permite analizar manualmente una rese√±a individual
- **`st.form`**: Agrupa los inputs y evita recargas prematuras
  - `clear_on_submit=True`: Limpia el formulario despu√©s de enviar
- **`st.selectbox`**: Lista desplegable con tipos de caf√© predefinidos
- **`st.text_area`**: Campo de texto multi-l√≠nea para el comentario
- **Proceso de an√°lisis**:
  1. Valida que el texto no est√© vac√≠o
  2. Llama a `ia.analizar(txt)` que retorna etiqueta y confianza
  3. Obtiene fecha y hora actual
  4. Inserta en base de datos con `db.insertar_resena()`
  5. Registra la operaci√≥n en el log
  6. Muestra resultados al usuario con formato visual

#### **Secci√≥n 6: M√ìDULO 2 - Carga Masiva CSV (L√≠neas 76-121)**

```python
elif menu == "Carga CSV":
    st.header("üìÅ Procesamiento Masivo de Archivos")
    st.info("Requisito: El CSV debe contener la columna 'comentario'.")

    archivo = st.file_uploader(
        "Subir CSV", type=["csv"], key=f"uploader_{st.session_state.uploader_key}"
    )
```

**Explicaci√≥n:**
- **Prop√≥sito**: Procesar m√∫ltiples rese√±as desde un archivo CSV
- **`st.file_uploader`**: Componente para subir archivos
  - Acepta solo archivos `.csv`
  - `key` din√°mico permite resetear el componente cuando se limpia la BD

```python
    if archivo:
        df = fm.leer_csv(archivo)
        df.columns = df.columns.str.strip().str.lower()

        if "comentario" not in df.columns:
            st.error("‚ùå Error: No se encontr√≥ la columna 'comentario' en el archivo.")
```

**Explicaci√≥n:**
- Lee el CSV usando el FileManager
- Normaliza nombres de columnas: elimina espacios y convierte a min√∫sculas
- Valida que exista la columna obligatoria `comentario`

```python
        else:
            df["comentario"] = df["comentario"].astype(str)
            if "fecha" in df.columns:
                df["fecha"] = pd.to_datetime(
                    df["fecha"], dayfirst=True, errors="coerce"
                )

            st.write("Vista previa de los datos:")
            st.dataframe(df.head(3), use_container_width=True)
```

**Explicaci√≥n:**
- Convierte la columna `comentario` a string para evitar errores
- Si existe columna `fecha`, la convierte a formato datetime
  - `dayfirst=True`: Asume formato DD/MM/YYYY
  - `errors="coerce"`: Fechas inv√°lidas se convierten a NaT (Not a Time)
- Muestra las primeras 3 filas como vista previa

```python
            if st.button("üöÄ Iniciar Procesamiento Masivo"):
                progreso = st.progress(0)
                total_filas = len(df)

                for i, r in df.iterrows():
                    label, score = ia.analizar(str(r["comentario"]))
                    f_val = r.get("fecha")
                    f_final = (
                        f_val.strftime("%d/%m/%Y %H:%M") if pd.notna(f_val) else None
                    )

                    db.insertar_resena(
                        r.get("producto", "Caf√©"),
                        r["comentario"],
                        label,
                        score,
                        f_final,
                    )
                    progreso.progress((i + 1) / total_filas)

                fm.registrar_log(f"Carga masiva: {total_filas} registros.")
                st.success(f"‚úÖ Carga completada: {total_filas} registros procesados.")
```

**Explicaci√≥n del Proceso:**
1. **`st.progress(0)`**: Crea barra de progreso inicializada en 0%
2. **Iteraci√≥n**: Recorre cada fila del DataFrame
   - `df.iterrows()`: Retorna √≠ndice y fila como Series
3. **An√°lisis**: Procesa cada comentario con el modelo de IA
4. **Formateo de fecha**: 
   - Si existe fecha v√°lida, la formatea a string
   - Si no existe o es inv√°lida, usa None
5. **Inserci√≥n**: Guarda en BD con producto (o "Caf√©" por defecto)
6. **Actualizaci√≥n de progreso**: Calcula porcentaje completado
7. **Registro final**: Log de la operaci√≥n y mensaje de √©xito

#### **Secci√≥n 7: M√ìDULO 3 - Base de Datos (L√≠neas 123-136)**

```python
elif menu == "Base de Datos":
    st.header("üíæ Gesti√≥n de Datos")
    df_data = db.obtener_datos()
    if not df_data.empty:
        st.write(f"Total de registros: **{len(df_data)}**")
        busqueda = st.text_input("üîç Buscar en comentarios o productos")
        if busqueda:
            df_data = df_data[
                df_data["comentario"].str.contains(busqueda, case=False)
                | df_data["producto"].str.contains(busqueda, case=False)
            ]
        st.dataframe(df_data, use_container_width=True, hide_index=True)
    else:
        st.info("La base de datos est√° vac√≠a.")
```

**Explicaci√≥n:**
- **Prop√≥sito**: Visualizar y buscar registros almacenados
- **`db.obtener_datos()`**: Recupera todos los registros como DataFrame
- **Contador**: Muestra el total de registros existentes
- **B√∫squeda**:
  - `st.text_input`: Campo de b√∫squeda en tiempo real
  - `str.contains`: Busca coincidencias parciales (case insensitive)
  - Operador `|`: OR l√≥gico - busca en comentarios O productos
- **Visualizaci√≥n**: Tabla interactiva que ocupa todo el ancho, sin √≠ndices

#### **Secci√≥n 8: M√ìDULO 4 - Dashboard Pro (L√≠neas 139-244)**

Este es el m√≥dulo m√°s complejo, que proporciona an√°lisis avanzado con visualizaciones.

##### **Parte A: Preparaci√≥n y Filtrado de Datos (L√≠neas 139-160)**

```python
elif menu == "Dashboard Pro":
    st.header("üìä An√°lisis de Sentimiento Avanzado")
    df_raw = db.obtener_datos()

    if not df_raw.empty:
        # Normalizaci√≥n de datos
        df_raw["fecha_dt"] = pd.to_datetime(
            df_raw["fecha"], dayfirst=True, errors="coerce"
        )
        df_raw["sentimiento"] = (
            df_raw["sentimiento"].fillna("NEU").astype(str).str.strip().str.upper()
        )
```

**Explicaci√≥n:**
- Recupera todos los datos de la base de datos
- **Normalizaci√≥n de fechas**: Convierte strings a objetos datetime
- **Normalizaci√≥n de sentimientos**:
  - `fillna("NEU")`: Valores nulos se marcan como NEUTRAL
  - Elimina espacios y convierte a may√∫sculas para consistencia

```python
        # Filtro de Rango en Sidebar
        st.sidebar.subheader("Filtros")
        min_d = df_raw["fecha_dt"].min().date()
        max_d = df_raw["fecha_dt"].max().date()
        rango = st.sidebar.date_input("Rango de An√°lisis", [min_d, max_d])

        if len(rango) == 2:
            inicio, fin = rango
            df_data = df_raw[
                (df_raw["fecha_dt"].dt.date >= inicio)
                & (df_raw["fecha_dt"].dt.date <= fin)
            ]
        else:
            df_data = df_raw
```

**Explicaci√≥n:**
- Extrae fechas m√≠nima y m√°xima del dataset
- `st.sidebar.date_input`: Widget de selecci√≥n de rango de fechas
  - Inicializado con el rango completo de datos
- **Filtrado**:
  - Valida que se hayan seleccionado 2 fechas (inicio y fin)
  - Filtra el DataFrame para incluir solo registros en ese rango
  - Si no hay rango v√°lido, usa todos los datos

##### **Parte B: KPIs - Indicadores Clave (L√≠neas 163-173)**

```python
        # KPIs
        kpi1, kpi2, kpi3 = st.columns(3)
        total = len(df_data)
        pos_count = len(df_data[df_data["sentimiento"] == "POS"])
        pos_perc = (pos_count / total) * 100 if total > 0 else 0

        kpi1.metric("Total Rese√±as", total)
        kpi2.metric("Sentimiento Positivo", f"{pos_perc:.1f}%")
        kpi3.metric("Confianza Promedio", f"{df_data['confianza'].mean():.2f}")
```

**Explicaci√≥n:**
- **`st.columns(3)`**: Divide la pantalla en 3 columnas iguales
- **C√°lculos**:
  - `total`: N√∫mero total de rese√±as en el rango seleccionado
  - `pos_count`: Cuenta de rese√±as positivas
  - `pos_perc`: Porcentaje de positividad (validaci√≥n contra divisi√≥n por cero)
- **M√©tricas visuales**:
  - Muestra cada KPI en su propia columna con formato grande
  - Formato num√©rico: 1 decimal para porcentaje, 2 para confianza

##### **Parte C: Gr√°fico de Distribuci√≥n Global - Pie Chart (L√≠neas 177-191)**

```python
        col_a, col_b = st.columns(2)
        colores_map = {"POS": "#2ecc71", "NEG": "#e74c3c", "NEU": "#f1c40f"}

        with col_a:
            st.subheader("Distribuci√≥n Global")
            counts = df_data["sentimiento"].value_counts()
            fig1, ax1 = plt.subplots()
            counts.plot(
                kind="pie",
                autopct="%1.1f%%",
                ax=ax1,
                colors=[colores_map.get(x, "#3498db") for x in counts.index],
            )
            ax1.set_ylabel("")  # Quita etiqueta 'count'
            st.pyplot(fig1)
            plt.close(fig1)
```

**Explicaci√≥n:**
- **Layout**: Divide en 2 columnas para gr√°ficos lado a lado
- **Paleta de colores**:
  - Verde (#2ecc71) para positivo
  - Rojo (#e74c3c) para negativo
  - Amarillo (#f1c40f) para neutral
- **Gr√°fico Circular (Pie)**:
  - `value_counts()`: Cuenta frecuencia de cada sentimiento
  - `autopct="%1.1f%%"`: Muestra porcentajes con 1 decimal
  - Aplica colores seg√∫n el diccionario de mapeo
  - `plt.close(fig1)`: Libera memoria despu√©s de renderizar

##### **Parte D: Gr√°fico de Barras Apiladas (L√≠neas 193-207)**

```python
        with col_b:
            st.subheader("Sentimiento por Producto")
            ct = pd.crosstab(df_data["producto"], df_data["sentimiento"])
            fig2, ax2 = plt.subplots()
            ct.plot(
                kind="bar",
                stacked=True,
                ax=ax2,
                color=[colores_map.get(col) for col in ct.columns],
            )
            ax2.set_xlabel("Productos")
            ax2.set_ylabel("Cantidad de Rese√±as")
            plt.xticks(rotation=45)
            plt.tight_layout()
            st.pyplot(fig2)
            plt.close(fig2)
```

**Explicaci√≥n:**
- **`pd.crosstab`**: Crea tabla de contingencia (matriz de frecuencias)
  - Filas: Productos
  - Columnas: Sentimientos
  - Valores: Conteo de ocurrencias
- **Gr√°fico de Barras Apiladas**:
  - `stacked=True`: Apila sentimientos en cada barra de producto
  - Permite comparar visualmente qu√© producto tiene mejor/peor sentimiento
  - `rotation=45`: Inclina etiquetas del eje X para mejor legibilidad
  - `tight_layout()`: Ajusta autom√°ticamente los m√°rgenes

##### **Parte E: Gr√°fico de Tendencia Temporal (L√≠neas 210-223)**

```python
        st.subheader("üìà Evoluci√≥n Diaria")
        df_trend = df_data.copy()
        df_trend["fecha_solo"] = df_trend["fecha_dt"].dt.date
        if not df_trend["fecha_solo"].dropna().empty:
            trend_pivot = (
                df_trend.groupby(["fecha_solo", "sentimiento"])
                .size()
                .unstack(fill_value=0)
            )
            # Ordenamos para asegurar colores: Rojo, Amarillo, Verde
            cols = [c for c in ["NEG", "NEU", "POS"] if c in trend_pivot.columns]
            st.line_chart(
                trend_pivot[cols],
                color=[colores_map.get(c) for c in cols],
                y_label="Rese√±as",
            )
```

**Explicaci√≥n:**
- **Preparaci√≥n de datos temporales**:
  - Extrae solo la fecha (sin hora) de cada timestamp
  - Crea copia para evitar modificar el DataFrame original
- **Agregaci√≥n**:
  - `groupby`: Agrupa por fecha y sentimiento
  - `.size()`: Cuenta registros por grupo
  - `.unstack()`: Pivotea sentimientos a columnas (fill_value=0 para fechas sin datos)
- **Gr√°fico de L√≠neas**:
  - Muestra evoluci√≥n temporal de cada sentimiento
  - Orden espec√≠fico de columnas (NEG, NEU, POS) para consistencia visual
  - `st.line_chart`: Componente nativo de Streamlit (interactivo)

##### **Parte F: Resumen Ejecutivo (L√≠neas 226-239)**

```python
        st.divider()
        st.subheader("üìã Resumen Ejecutivo")
        r1, r2 = st.columns(2)
        prod_pos = df_data[df_data["sentimiento"] == "POS"]["producto"].value_counts()
        prod_neg = df_data[df_data["sentimiento"] == "NEG"]["producto"].value_counts()

        with r1:
            if not prod_pos.empty:
                st.success(f"üåü **Producto Estrella:** {prod_pos.idxmax()}")
        with r2:
            if not prod_neg.empty:
                st.error(f"‚ö†Ô∏è **Punto de Mejora:** {prod_neg.idxmax()}")
    else:
        st.info("No hay datos para el rango seleccionado.")
```

**Explicaci√≥n:**
- **An√°lisis de productos**:
  - `prod_pos`: Cuenta rese√±as positivas por producto
  - `prod_neg`: Cuenta rese√±as negativas por producto
- **Insights de negocio**:
  - `idxmax()`: Encuentra el producto con m√°s rese√±as positivas/negativas
  - **Producto Estrella**: El que tiene m√°s opiniones positivas (√©xito)
  - **Punto de Mejora**: El que tiene m√°s opiniones negativas (oportunidad)
- **Presentaci√≥n visual**:
  - Box verde (success) para el producto estrella
  - Box rojo (error) para √°rea de mejora
  - Validaci√≥n de datos vac√≠os antes de mostrar

---

### 2Ô∏è‚É£ `models/database.py` - Gesti√≥n de Base de Datos

Este m√≥dulo encapsula todas las operaciones relacionadas con SQLite.

#### **Clase StarcuakDB**

```python
class StarcuakDB:
    def __init__(self, db_path="data/starcuak.db"):
        self.db_path = db_path
        self._crear_tabla()
```

**Explicaci√≥n:**
- **Constructor**: Se ejecuta al instanciar la clase
- `db_path`: Ruta donde se almacena la base de datos SQLite
  - Por defecto: `data/starcuak.db`
  - Almacena la ruta como atributo de instancia
- `_crear_tabla()`: M√©todo privado que inicializa la estructura de la BD

#### **M√©todo `_crear_tabla`**

```python
def _crear_tabla(self):
    """Crea la estructura relacional inicial."""
    try:
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS Resenas (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    producto TEXT,
                    comentario TEXT,
                    sentimiento TEXT,
                    confianza REAL,
                    fecha TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)
            conn.commit()
    except sqlite3.Error as e:
        print(f"Error de base de datos: {e}")
```

**Explicaci√≥n:**
- **Context Manager** (`with`): Garantiza cierre autom√°tico de conexi√≥n
- **Schema de la tabla Resenas**:
  - `id`: Clave primaria autoincremental (√∫nico identificador)
  - `producto`: Tipo de caf√© (TEXT, sin restricciones)
  - `comentario`: Texto de la rese√±a (TEXT, sin l√≠mite)
  - `sentimiento`: Clasificaci√≥n (POS/NEG/NEU)
  - `confianza`: Score del modelo de IA (REAL = float)
  - `fecha`: Timestamp con valor por defecto
- **`CREATE TABLE IF NOT EXISTS`**: Solo crea si no existe (idempotente)
- **Manejo de errores**: Captura excepciones espec√≠ficas de SQLite

#### **M√©todo `insertar_resena`**

```python
def insertar_resena(self, producto, comentario, sentimiento, confianza, fecha=None):
    """Realiza operaciones DML de inserci√≥n."""
    try:
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            query = "INSERT INTO Resenas (producto, comentario, sentimiento, confianza, fecha) VALUES (?, ?, ?, ?, ?)"
            cursor.execute(
                query, (producto, comentario, sentimiento, confianza, fecha)
            )
            conn.commit()
    except sqlite3.Error as e:
        raise Exception(f"Fallo en la persistencia DML: {e}")
```

**Explicaci√≥n:**
- **Par√°metros**: Recibe todos los campos de una rese√±a
  - `fecha=None`: Par√°metro opcional (puede ser NULL en BD)
- **Queries parametrizadas** (`?`): Previenen SQL Injection
  - Los valores se pasan como tupla separada de la query
- **`conn.commit()`**: Persiste los cambios en disco
- **Manejo de errores**: 
  - Captura errores de SQLite
  - Lanza una excepci√≥n gen√©rica con mensaje descriptivo
  - Permite que el c√≥digo llamador maneje el error

#### **M√©todo `obtener_datos`**

```python
def obtener_datos(self):
    """Recupera datos para el dashboard."""
    with sqlite3.connect(self.db_path) as conn:
        return pd.read_sql("SELECT * FROM Resenas", conn)
```

**Explicaci√≥n:**
- **Prop√≥sito**: Recuperar todos los registros para visualizaci√≥n
- **`pd.read_sql`**: Ejecuta query SQL y retorna DataFrame de pandas
  - Convierte autom√°ticamente tipos de datos SQL a Python
  - Usa nombres de columnas de la BD como nombres de columnas del DF
- **Eficiencia**: Recupera todos los campos con `SELECT *`
  - En producci√≥n, ser√≠a mejor especificar columnas necesarias

#### **M√©todo `limpiar_datos`**

```python
def limpiar_datos(self):
    """Elimina todos los registros y reinicia el contador de ID."""
    try:
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            # 1. Eliminamos los datos de la tabla
            cursor.execute("DELETE FROM Resenas")
            # 2. Reiniciamos el contador de ID autoincremental de SQLite
            cursor.execute("DELETE FROM sqlite_sequence WHERE name='Resenas'")
            conn.commit()
    except sqlite3.Error as e:
        raise Exception(f"Error al limpiar la base de datos: {e}")
```

**Explicaci√≥n:**
- **Doble operaci√≥n de limpieza**:
  1. `DELETE FROM Resenas`: Elimina todas las filas
  2. `DELETE FROM sqlite_sequence`: Reinicia el contador de autoincremento
- **sqlite_sequence**: Tabla del sistema que almacena contadores AUTOINCREMENT
- **Importancia del reset**: Sin el segundo DELETE, los nuevos IDs continuar√≠an desde el √∫ltimo valor
- **Transaccional**: Ambas operaciones se confirman juntas (atomicidad)

---

### 3Ô∏è‚É£ `models/ia_model.py` - Modelo de Inteligencia Artificial

Este m√≥dulo implementa el an√°lisis de sentimientos usando transformers.

#### **Clase AnalizadorIA**

```python
class AnalizadorIA:
    def __init__(self):
        try:
            # Modelo preentrenado optimizado para sentimientos
            self.model = pipeline(
                "sentiment-analysis", model="finiteautomata/beto-sentiment-analysis"
            )
        except Exception:
            self.model = pipeline("sentiment-analysis")
```

**Explicaci√≥n:**
- **`pipeline`**: API de alto nivel de Hugging Face Transformers
  - Simplifica el uso de modelos preentrenados
  - Maneja tokenizaci√≥n, inferencia y post-procesamiento autom√°ticamente
- **Modelo BETO**:
  - `finiteautomata/beto-sentiment-analysis`: Modelo especializado en espa√±ol
  - Basado en BERT entrenado en corpus espa√±ol
  - Optimizado espec√≠ficamente para an√°lisis de sentimientos
- **Fallback**: Si falla la carga de BETO (red, descarga, etc.):
  - Carga el modelo por defecto de Hugging Face (ingl√©s)
  - Garantiza que la aplicaci√≥n funcione aunque con menor precisi√≥n en espa√±ol
- **Descarga autom√°tica**: Primera ejecuci√≥n descarga el modelo (~400MB)
  - Descargas subsecuentes usan cach√© local

#### **M√©todo `analizar`**

```python
def analizar(self, texto):
    """Aplica el modelo y devuelve resultados interpretables."""
    if not texto.strip():
        return "NEUTRAL", 0.0
    resultado = self.model(texto)[0]
    return resultado["label"], resultado["score"]
```

**Explicaci√≥n:**
- **Validaci√≥n de entrada**: 
  - `texto.strip()`: Elimina espacios al inicio y final
  - Textos vac√≠os retornan NEUTRAL con confianza 0
  - Evita errores de procesamiento con el modelo
- **Inferencia**:
  - `self.model(texto)`: Ejecuta el modelo sobre el texto
  - Retorna lista de diccionarios (uno por entrada)
  - `[0]`: Toma el primer resultado (an√°lisis de una sola entrada)
- **Extracci√≥n de resultados**:
  - `label`: Etiqueta del sentimiento (POS/NEG/NEU)
  - `score`: Confianza del modelo (0.0 a 1.0)
- **Formato de retorno**: Tupla `(etiqueta, confianza)`
  - Facilita desempaquetado: `label, score = ia.analizar(texto)`

**Notas sobre el modelo BETO:**
- **Arquitectura**: BERT (Bidirectional Encoder Representations from Transformers)
- **Idioma**: Optimizado para espa√±ol latinoamericano y peninsular
- **Entrenamiento**: Corpus masivos de textos en espa√±ol
- **Clases de salida**:
  - POS (Positivo)
  - NEG (Negativo)
  - NEU (Neutral) - depende de la versi√≥n del modelo
- **Performance**: T√≠picamente > 85% de precisi√≥n en datasets de sentimientos

---

### 4Ô∏è‚É£ `models/file_manager.py` - Gesti√≥n de Archivos

Este m√≥dulo centraliza operaciones de I/O (entrada/salida) de archivos.

#### **Clase FileManager**

Utiliza m√©todos est√°ticos ya que no requiere mantener estado interno.

#### **M√©todo `leer_csv`**

```python
@staticmethod
def leer_csv(ruta):
    """Lectura de datos estructurados CSV con autodetecci√≥n de separador."""
    try:
        # sep=None permite que pandas detecte si es coma (,) o punto y coma (;)
        return pd.read_csv(ruta, sep=None, engine="python", encoding="utf-8-sig")
    except Exception as e:
        raise Exception(f"Error al leer el archivo CSV: {e}")
```

**Explicaci√≥n:**
- **`@staticmethod`**: M√©todo que no requiere instancia de la clase
  - Puede llamarse como `FileManager.leer_csv()` sin crear objeto
- **Par√°metros de `read_csv`**:
  - `sep=None`: Pandas detecta autom√°ticamente el separador
    - Detecta comas (,), punto y coma (;), tabuladores, etc.
  - `engine="python"`: Motor de parsing m√°s flexible
    - Soporta expresiones regulares y detecci√≥n autom√°tica
    - M√°s lento que 'c' pero m√°s robusto
  - `encoding="utf-8-sig"`: Maneja BOM (Byte Order Mark) de UTF-8
    - Com√∫n en archivos generados por Excel
    - Elimina caracteres invisibles al inicio del archivo
- **Manejo de errores**:
  - Captura cualquier excepci√≥n (archivo no encontrado, formato inv√°lido, etc.)
  - Propaga excepci√≥n con mensaje descriptivo

#### **M√©todo `guardar_binario`**

```python
@staticmethod
def guardar_binario(datos, ruta="data/outputs/backup_starcuak.dat"):
    """Persistencia en formato binario."""
    with open(ruta, "wb") as f:
        pickle.dump(datos, f)
```

**Explicaci√≥n:**
- **Prop√≥sito**: Serializar objetos Python a disco
- **`pickle`**: M√≥dulo est√°ndar de Python para serializaci√≥n
  - Convierte objetos Python a secuencia de bytes
  - Preserva estructuras de datos complejas (listas, dicts, objetos)
- **Modo de apertura** (`"wb"`):
  - `w`: Write (escritura)
  - `b`: Binary (modo binario)
- **Casos de uso**:
  - Backups de DataFrames procesados
  - Guardar configuraciones complejas
  - Cach√© de resultados computacionalmente costosos
- **‚ö†Ô∏è Advertencia de seguridad**:
  - Solo cargar pickles de fuentes confiables
  - Pueden ejecutar c√≥digo arbitrario al deserializar

#### **M√©todo `registrar_log`**

```python
@staticmethod
def registrar_log(mensaje, ruta="data/outputs/log.txt"):
    """Registro de operaciones en archivo de texto."""
    fecha = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    with open(ruta, "a") as f:
        f.write(f"[{fecha}] {mensaje}\n")
```

**Explicaci√≥n:**
- **Prop√≥sito**: Sistema de logging simple basado en archivos
- **Formato del timestamp**:
  - `%Y-%m-%d %H:%M:%S`: Formato ISO 8601 (ordenable lexicogr√°ficamente)
  - Ejemplo: `[2026-01-21 14:35:22] An√°lisis Manual: Latte -> POS`
- **Modo de apertura** (`"a"`):
  - Append: A√±ade al final sin borrar contenido existente
  - Crea el archivo si no existe
- **Formato del mensaje**:
  - `[timestamp] mensaje\n`
  - Newline al final para separar entradas
- **Ventajas**:
  - Auditor√≠a de operaciones
  - Debugging de problemas en producci√≥n
  - An√°lisis de patrones de uso
- **Mejoras potenciales**:
  - Rotaci√≥n de logs (por tama√±o/fecha)
  - Niveles de severidad (INFO, WARNING, ERROR)
  - Uso de m√≥dulo `logging` est√°ndar de Python

---

### 5Ô∏è‚É£ `models/__init__.py` - Inicializador del Paquete

```python
from .database import StarcuakDB
from .ia_model import AnalizadorIA
from .file_manager import FileManager

__all__ = ["StarcuakDB", "AnalizadorIA", "FileManager"]
```

**Explicaci√≥n:**
- **Prop√≥sito**: Convierte el directorio `models/` en un paquete Python
- **Imports relativos** (`.`):
  - Importa clases desde m√≥dulos hermanos dentro del mismo paquete
  - `.database` equivale a `models.database`
- **`__all__`**: Define la API p√∫blica del paquete
  - Lista de nombres que se exportan con `from models import *`
  - Buena pr√°ctica para control de namespaces
- **Ventajas**:
  - Simplifica imports en otros archivos
  - En vez de: `from models.database import StarcuakDB`
  - Se puede usar: `from models import StarcuakDB`
- **Centralizaci√≥n**: Punto √∫nico de entrada al paquete models

---

### 6Ô∏è‚É£ `requirements.txt` - Dependencias

```
streamlit
pandas
transformers
torch
matplotlib
```

**Explicaci√≥n detallada de cada dependencia:**

#### **streamlit**
- **Versi√≥n actual**: 1.x
- **Prop√≥sito**: Framework para crear aplicaciones web interactivas
- **Caracter√≠sticas usadas**:
  - `st.sidebar`: Barra lateral de navegaci√≥n
  - `st.form`: Formularios con control de env√≠o
  - `st.dataframe`: Visualizaci√≥n tabular interactiva
  - `st.pyplot`: Integraci√≥n con Matplotlib
  - `st.line_chart`: Gr√°ficos de l√≠neas nativos
  - `st.session_state`: Manejo de estado de la aplicaci√≥n
- **Instalaci√≥n**: ~15 MB

#### **pandas**
- **Versi√≥n recomendada**: >= 1.3.0
- **Prop√≥sito**: Manipulaci√≥n y an√°lisis de datos estructurados
- **Uso en el proyecto**:
  - DataFrames para almacenar datos de BD
  - Operaciones de filtrado y agregaci√≥n
  - `crosstab`: Tablas de contingencia
  - `read_csv`: Lectura de archivos CSV
  - `read_sql`: Integraci√≥n con SQLite
  - Conversi√≥n de tipos de datos
- **Instalaci√≥n**: ~30 MB

#### **transformers**
- **Desarrollador**: Hugging Face
- **Versi√≥n recomendada**: >= 4.0.0
- **Prop√≥sito**: Modelos de lenguaje basados en transformers
- **Uso en el proyecto**:
  - `pipeline`: API simplificada para an√°lisis de sentimientos
  - Descarga y gesti√≥n de modelos preentrenados
  - Tokenizaci√≥n y procesamiento de texto
- **Modelos usados**: BETO (BERT espa√±ol)
- **Instalaci√≥n**: ~10 MB (sin modelos)
- **Nota**: Primer uso descarga modelo (~400 MB)

#### **torch** (PyTorch)
- **Versi√≥n recomendada**: >= 1.9.0
- **Prop√≥sito**: Framework de deep learning
- **Por qu√© es necesario**:
  - Backend para ejecutar modelos de transformers
  - Operaciones tensoriales de alto rendimiento
  - Gesti√≥n de GPU (opcional)
- **Uso en el proyecto**:
  - Indirectamente a trav√©s de transformers
  - Inferencia del modelo BETO
- **Instalaci√≥n**: ~700 MB (CPU), ~2 GB (GPU)
- **Alternativas**: TensorFlow (no compatible con este proyecto)

#### **matplotlib**
- **Versi√≥n recomendada**: >= 3.3.0
- **Prop√≥sito**: Visualizaci√≥n de datos
- **Uso en el proyecto**:
  - Gr√°ficos de pastel (pie charts)
  - Gr√°ficos de barras apiladas
  - Integraci√≥n con Streamlit via `st.pyplot`
- **M√≥dulos usados**:
  - `pyplot`: API similar a MATLAB
- **Instalaci√≥n**: ~15 MB

**Tama√±o total estimado**: ~1.5 GB (incluyendo modelo BETO)

**Instalaci√≥n del proyecto**:
```bash
pip install -r requirements.txt
```

---

## üîÑ Flujo de Datos

### Diagrama de Flujo - An√°lisis de Rese√±a Individual

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Usuario ingresa comentario en formulario (app.py)      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Validaci√≥n: ¬øTexto no vac√≠o?                           ‚îÇ
‚îÇ - S√≠: Continuar                                         ‚îÇ
‚îÇ - No: Mostrar advertencia y detener                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ AnalizadorIA.analizar(texto)                           ‚îÇ
‚îÇ 1. Tokenizaci√≥n del texto                              ‚îÇ
‚îÇ 2. Conversi√≥n a tensores                               ‚îÇ
‚îÇ 3. Inferencia con modelo BETO                          ‚îÇ
‚îÇ 4. Obtenci√≥n de logits                                 ‚îÇ
‚îÇ 5. Aplicaci√≥n de softmax                               ‚îÇ
‚îÇ 6. Retorno de (label, score)                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Preparaci√≥n de datos para BD:                          ‚îÇ
‚îÇ - producto: selecci√≥n del usuario                      ‚îÇ
‚îÇ - comentario: texto ingresado                          ‚îÇ
‚îÇ - sentimiento: label del modelo                        ‚îÇ
‚îÇ - confianza: score del modelo                          ‚îÇ
‚îÇ - fecha: timestamp actual                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ StarcuakDB.insertar_resena(...)                        ‚îÇ
‚îÇ 1. Conexi√≥n a SQLite                                   ‚îÇ
‚îÇ 2. Preparaci√≥n de query parametrizada                  ‚îÇ
‚îÇ 3. Ejecuci√≥n de INSERT                                 ‚îÇ
‚îÇ 4. Commit de transacci√≥n                               ‚îÇ
‚îÇ 5. Cierre de conexi√≥n                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ FileManager.registrar_log(mensaje)                     ‚îÇ
‚îÇ 1. Obtenci√≥n de timestamp                              ‚îÇ
‚îÇ 2. Formateo de mensaje                                 ‚îÇ
‚îÇ 3. Append a data/outputs/log.txt                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Mostrar resultados al usuario (Streamlit)              ‚îÇ
‚îÇ - Info box: Comentario procesado                       ‚îÇ
‚îÇ - Success box: Sentimiento y confianza                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Diagrama de Flujo - Carga Masiva CSV

```
Usuario sube archivo CSV
        ‚îÇ
        ‚ñº
FileManager.leer_csv()
        ‚îÇ
        ‚îú‚îÄ Autodetecci√≥n de separador
        ‚îú‚îÄ Detecci√≥n de encoding
        ‚îî‚îÄ Retorno de DataFrame
        ‚îÇ
        ‚ñº
Validaci√≥n: ¬øColumna 'comentario' existe?
        ‚îÇ
        ‚îú‚îÄ No ‚Üí Mostrar error y detener
        ‚îÇ
        ‚îú‚îÄ S√≠ ‚Üí Continuar
        ‚îÇ
        ‚ñº
Normalizaci√≥n de datos:
        ‚îÇ
        ‚îú‚îÄ Columnas a min√∫sculas
        ‚îú‚îÄ Comentarios a string
        ‚îî‚îÄ Fechas a datetime (si existe columna)
        ‚îÇ
        ‚ñº
Mostrar vista previa (3 filas)
        ‚îÇ
        ‚ñº
Usuario presiona "Iniciar Procesamiento"
        ‚îÇ
        ‚ñº
Bucle para cada fila:
        ‚îÇ
        ‚îú‚îÄ Analizar comentario con IA
        ‚îú‚îÄ Formatear fecha
        ‚îú‚îÄ Insertar en BD
        ‚îî‚îÄ Actualizar barra de progreso
        ‚îÇ
        ‚ñº
FileManager.registrar_log()
        ‚îÇ
        ‚ñº
Mostrar mensaje de √©xito con total de registros
```

### Diagrama de Flujo - Dashboard

```
Carga de m√≥dulo Dashboard Pro
        ‚îÇ
        ‚ñº
StarcuakDB.obtener_datos()
        ‚îÇ
        ‚îú‚îÄ Consulta: SELECT * FROM Resenas
        ‚îî‚îÄ Retorno de DataFrame
        ‚îÇ
        ‚ñº
Normalizaci√≥n:
        ‚îÇ
        ‚îú‚îÄ Conversi√≥n de fechas a datetime
        ‚îî‚îÄ Normalizaci√≥n de sentimientos a uppercase
        ‚îÇ
        ‚ñº
Configuraci√≥n de filtros:
        ‚îÇ
        ‚îú‚îÄ C√°lculo de rango de fechas (min/max)
        ‚îî‚îÄ Widget de selecci√≥n de rango
        ‚îÇ
        ‚ñº
Aplicaci√≥n de filtros seleccionados
        ‚îÇ
        ‚ñº
C√°lculo de KPIs:
        ‚îÇ
        ‚îú‚îÄ Total de rese√±as
        ‚îú‚îÄ Porcentaje positivo
        ‚îî‚îÄ Confianza promedio
        ‚îÇ
        ‚ñº
Generaci√≥n de visualizaciones:
        ‚îÇ
        ‚îú‚îÄ Pie chart: Distribuci√≥n global
        ‚îú‚îÄ Bar chart: Sentimiento por producto
        ‚îî‚îÄ Line chart: Evoluci√≥n temporal
        ‚îÇ
        ‚ñº
An√°lisis ejecutivo:
        ‚îÇ
        ‚îú‚îÄ Identificar producto con m√°s positivos
        ‚îî‚îÄ Identificar producto con m√°s negativos
        ‚îÇ
        ‚ñº
Renderizado en interfaz Streamlit
```

---

## üöÄ Instalaci√≥n y Configuraci√≥n

### Requisitos del Sistema

- **Python**: >= 3.8
- **Sistema Operativo**: Windows, Linux, macOS
- **RAM recomendada**: >= 4 GB
- **Espacio en disco**: >= 2 GB (para dependencias y modelo)
- **Conexi√≥n a internet**: Requerida para primera ejecuci√≥n (descarga de modelo)

### Pasos de Instalaci√≥n

#### 1. Clonar o descargar el proyecto

```bash
git clone <repositorio>
cd PROYECTO_STARCUAK
```

#### 2. Crear entorno virtual (recomendado)

```bash
# Windows
python -m venv venv
venv\Scripts\activate

# Linux/Mac
python3 -m venv venv
source venv/bin/activate
```

#### 3. Instalar dependencias

```bash
pip install -r requirements.txt
```

**Nota**: Primera instalaci√≥n puede tomar 10-15 minutos dependiendo de la velocidad de conexi√≥n.

#### 4. Verificar estructura de directorios

Asegurarse de que existan los siguientes directorios:
```bash
mkdir -p data/inputs data/outputs
```

#### 5. Ejecutar la aplicaci√≥n

```bash
streamlit run app.py
```

La aplicaci√≥n se abrir√° autom√°ticamente en el navegador en:
```
http://localhost:8501
```

### Soluci√≥n de Problemas Comunes

#### Error: "No module named 'transformers'"
```bash
pip install transformers torch
```

#### Error: "torch not compiled with CUDA"
- **Soluci√≥n**: Esto es normal si no tienes GPU NVIDIA
- El modelo funcionar√° con CPU (m√°s lento pero funcional)
- Para habilitar GPU, instalar versi√≥n CUDA de PyTorch:
```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

#### Error al descargar modelo BETO
- **Causa**: Problemas de conexi√≥n o firewall
- **Soluci√≥n**: Descargar manualmente y colocar en cache de Hugging Face:
```bash
python -c "from transformers import pipeline; pipeline('sentiment-analysis', model='finiteautomata/beto-sentiment-analysis')"
```

#### Base de datos bloqueada (SQLite locked)
- **Causa**: M√∫ltiples instancias accediendo a la BD
- **Soluci√≥n**: Cerrar todas las instancias de Streamlit y reiniciar

---

## üíº Casos de Uso

### Caso de Uso 1: An√°lisis de Rese√±a Individual

**Escenario**: Un gerente quiere analizar manualmente un comentario espec√≠fico de un cliente.

**Pasos**:
1. Navegar a "Nueva Rese√±a"
2. Seleccionar el producto (ej: "Latte")
3. Ingresar el comentario: "El caf√© est√° delicioso pero el servicio fue lento"
4. Presionar "Analizar"

**Resultado esperado**:
- Sentimiento: NEU o POS (depende del modelo)
- Confianza: ~0.65-0.75
- Registro guardado en BD con timestamp

### Caso de Uso 2: Carga Masiva de Encuestas

**Escenario**: Se reciben 500 rese√±as de una encuesta de satisfacci√≥n en CSV.

**Estructura del CSV**:
```csv
producto,comentario,fecha
Espresso,Excelente caf√©,15/01/2026
Americano,Muy amargo,15/01/2026
Latte,Perfecta temperatura,16/01/2026
```

**Pasos**:
1. Navegar a "Carga CSV"
2. Arrastrar o seleccionar el archivo
3. Verificar vista previa
4. Presionar "Iniciar Procesamiento Masivo"
5. Esperar barra de progreso

**Resultado esperado**:
- 500 registros procesados
- Cada comentario analizado y clasificado
- Log generado con timestamp de la operaci√≥n

### Caso de Uso 3: An√°lisis de Tendencias

**Escenario**: El equipo de calidad quiere identificar si las mejoras implementadas en diciembre mejoraron la percepci√≥n.

**Pasos**:
1. Navegar a "Dashboard Pro"
2. En sidebar, seleccionar rango: 01/12/2025 - 31/12/2025
3. Observar:
   - KPI de sentimiento positivo
   - Gr√°fico de evoluci√≥n diaria
   - Identificar producto estrella

**Insights posibles**:
- "Capuccino muestra 78% de sentimiento positivo"
- "Tendencia ascendente en la segunda quincena de diciembre"
- "Espresso identificado como punto de mejora (40% negativos)"

### Caso de Uso 4: Auditor√≠a de Operaciones

**Escenario**: Se necesita verificar qu√© operaciones se realizaron en el sistema.

**Pasos**:
1. Acceder al archivo `data/outputs/log.txt`
2. Buscar entradas por fecha/operaci√≥n

**Ejemplo de entradas de log**:
```
[2026-01-21 09:15:22] Analisis Manual: Latte -> POS
[2026-01-21 09:30:45] Carga masiva: 150 registros.
[2026-01-21 10:05:12] Base de datos limpiada por el usuario.
```

### Caso de Uso 5: Limpieza de Datos de Prueba

**Escenario**: Despu√©s de demostraci√≥n o testing, se necesita limpiar la BD.

**Pasos**:
1. Presionar bot√≥n "üóëÔ∏è Limpiar Base de Datos" en sidebar
2. Confirmar acci√≥n
3. Sistema redirige autom√°ticamente a "Base de Datos"

**Resultado**:
- Tabla Resenas vac√≠a
- IDs reiniciados a 1
- Log registra la operaci√≥n

---

## üîê Consideraciones de Seguridad

### Datos Sensibles
- Los comentarios pueden contener informaci√≥n personal (PII)
- Recomendaci√≥n: Anonimizar datos antes de an√°lisis
- Cumplir con GDPR/LOPD si aplicable

### SQLite
- No usar en ambientes multi-usuario concurrentes
- Para producci√≥n, migrar a PostgreSQL/MySQL

### Modelo de IA
- BETO puede tener sesgos del corpus de entrenamiento
- Validar resultados en casos cr√≠ticos
- No usar para decisiones automatizadas sin supervisi√≥n humana

---

## üìä M√©tricas de Rendimiento

### Tiempos de Procesamiento (Promedio en CPU i5, 8GB RAM)

| Operaci√≥n | Tiempo |
|-----------|--------|
| An√°lisis de 1 rese√±a | ~0.3 segundos |
| An√°lisis de 100 rese√±as | ~30 segundos |
| An√°lisis de 1000 rese√±as | ~5 minutos |
| Carga de dashboard (1000 reg) | ~1 segundo |
| Query SQL simple | ~0.01 segundos |

### Optimizaciones Posibles

1. **Batch Processing**: Procesar m√∫ltiples rese√±as en un solo paso del modelo
2. **GPU Acceleration**: Usar CUDA para reducir tiempo de inferencia en 10x
3. **Cach√© de Resultados**: Guardar an√°lisis ya procesados
4. **Indexaci√≥n de BD**: Crear √≠ndices en columnas de fecha y producto

---

## üîÑ Mantenimiento y Actualizaci√≥n

### Actualizaciones de Dependencias

```bash
# Verificar versiones actuales
pip list

# Actualizar todas las dependencias
pip install --upgrade -r requirements.txt
```

### Backup de Base de Datos

```bash
# Backup manual
cp data/starcuak.db data/backups/starcuak_backup_$(date +%Y%m%d).db

# Restauraci√≥n
cp data/backups/starcuak_backup_20260121.db data/starcuak.db
```

### Rotaci√≥n de Logs

El archivo `log.txt` puede crecer indefinidamente. Implementar rotaci√≥n:

```python
# Ejemplo de rotaci√≥n manual
import os
from datetime import datetime

log_file = "data/outputs/log.txt"
if os.path.getsize(log_file) > 10_000_000:  # 10 MB
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    os.rename(log_file, f"data/outputs/log_{timestamp}.txt")
```

---

## üìö Referencias y Recursos

### Documentaci√≥n Oficial

- **Streamlit**: https://docs.streamlit.io/
- **Pandas**: https://pandas.pydata.org/docs/
- **Transformers**: https://huggingface.co/docs/transformers/
- **PyTorch**: https://pytorch.org/docs/
- **Matplotlib**: https://matplotlib.org/stable/contents.html
- **SQLite**: https://www.sqlite.org/docs.html

### Modelo BETO

- **Paper**: "Spanish Pre-Trained BERT Model and Evaluation Data"
- **Hugging Face**: https://huggingface.co/finiteautomata/beto-sentiment-analysis
- **Precisi√≥n**: ~87% en benchmark SemEval

### Recursos de Aprendizaje

- **Streamlit Tutorial**: https://streamlit.io/gallery
- **An√°lisis de Sentimientos**: https://nlp.stanford.edu/sentiment/
- **BERT Explained**: http://jalammar.github.io/illustrated-bert/

---

## ü§ù Contribuciones y Desarrollo Futuro

### Mejoras Propuestas

1. **Autenticaci√≥n de Usuarios**: Sistema de login para multi-usuario
2. **Exportaci√≥n de Reportes**: PDF/Excel con an√°lisis completo
3. **API REST**: Endpoints para integraci√≥n con otros sistemas
4. **An√°lisis Multiling√ºe**: Soporte para otros idiomas
5. **Detecci√≥n de Aspectos**: Identificar sobre qu√© se opina (precio, calidad, servicio)
6. **Alertas Autom√°ticas**: Notificaciones cuando sentimiento negativo supera umbral
7. **Integraci√≥n con Redes Sociales**: An√°lisis de Twitter, Facebook, Instagram

### Arquitectura Escalable (Futura)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Frontend   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   API REST  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Microserv. ‚îÇ
‚îÇ  (React)    ‚îÇ     ‚îÇ  (FastAPI)  ‚îÇ     ‚îÇ  IA/An√°lisis‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
                            ‚ñº
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ  PostgreSQL ‚îÇ
                    ‚îÇ  + Redis    ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìû Soporte

Para dudas o problemas:
- Revisar secci√≥n de "Soluci√≥n de Problemas"
- Consultar logs en `data/outputs/log.txt`
- Verificar versiones de dependencias con `pip list`

---

**Versi√≥n de Documentaci√≥n**: 1.0  
**√öltima Actualizaci√≥n**: 21 de enero de 2026  
**Autor del Proyecto**: JoseMartinez-AI  
**Licencia**: [Especificar licencia]

---

## üìù Glosario

- **BERT**: Bidirectional Encoder Representations from Transformers
- **BETO**: BERT Espa√±ol, versi√≥n del modelo para idioma espa√±ol
- **CSV**: Comma-Separated Values, formato de archivo de datos
- **DataFrame**: Estructura de datos tabular de pandas
- **KPI**: Key Performance Indicator, indicador clave de desempe√±o
- **NLP**: Natural Language Processing, procesamiento de lenguaje natural
- **Sentimiento**: Clasificaci√≥n emocional de un texto (positivo/negativo/neutral)
- **SQLite**: Sistema de gesti√≥n de base de datos relacional embebido
- **Streamlit**: Framework Python para aplicaciones web de datos
- **Transformer**: Arquitectura de red neuronal basada en mecanismos de atenci√≥n

---

